\section{General Information Useful for Training}
In the following chapters we will make use of different strategies:
\begin{itemize}
\item Class Weights (already talked about)
\item Data augmentation
\item Regularization
\item Dropout
\item Multiple activation functions
\item Multiple optimizers
\item Genetic Algorithms
\end{itemize}
In order to allow a better and faster reading of the tests done, in the following paragraph the mentioned strategies are discussed.

\subsection{Data Augmentation}
Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that, at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data so it can generalize better.
In Keras, this can be done by adding a number of data augmentation layers at the start of your model. In our model, we included the following transformation:

\begin{python}
data_augmentation = ks.Sequential(
    [
        layers.RandomFlip('horizontal'),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2),
        layers.RandomHeight(0.1),
        layers.RandomWidth(0.1)
    ]
)
\end{python}

\subsection{Regularization}
Regularization techniques are a set of best practices that actively impede the model’s ability to fit perfectly to the training data, with the goal of making the model perform better during validation. This is called “regularizing” the model, because it tends to make the model simpler, more “regular”, its curve smoother, more “generic”; thus it is less specific to the training set and better able to generalize by more closely approximating the latent manifold of the data.
A common way to mitigate overfitting is to put constraints on the complexity of a model by forcing its weights to take only small values, which makes the distribution of weight values more regular. This is called \textit{weight regularization}, and it’s done by adding to the loss function of the model a cost associated with having large weights. This cost comes in two flavors:
\begin{enumerate}
\item \textit{L1 regularization}—The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights).
\item \textit{L2 regularization}—The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights).
\item \textit{L1\_L2 regularization}—Combine L1 and L2.
\end{enumerate}

\subsection{Dropout}
Dropout is one of the most effective and most commonly used regularization techniques for neural networks; it was developed by Geoff Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training.

In \textbf{keras} can be set using the \textit{layers.Dropout} function passing as parameter the \textit{dropout rate}. We tried different values for the dropout rate during our studies, anyway for the \textit{Pre-Trained Models} chapters it is always set to 0.5 if not otherwise specified.

\subsection{Activation Functions}
In the studies done in the following chapters we used three different activation functions:
\begin{itemize}
	\item \textit{ReLU}: 
	$ \max (0,x) $
	\item \textit{ELU}: $ \max (0.2x,x) $
%	\item \textit{Leaky ReLU}: 
%	$ 
%		f(x) = \begin{cases}
%		x \ \ \ \ \ \ \ \ \ \ \ x>0 \\
%		\alpha(\exp(x) -1) \ \ \ \ \ x\le0
%		\end{cases}
%	$
\end{itemize}
They will be useful in the genetic algorithm analysis done fore the \textit{scratch architecture} and the \textit{VGG16}

\subsection{Optimizers}
An optimizer is the mechanism through which the model will update itself based on the training data it sees, so as to improve its performance. In our project we make use of:

\begin{itemize}
	\item \textit{RMSprop}: the gist of RMSprop is to:
		\subitem - Maintain a moving (discounted) average of the square of gradients
		\subitem - Divide the gradient by the root of this average
		\subitem - It uses plain momentum, not Nesterov momentum.
	\item \textit{Adam}: stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.
\end{itemize}

\subsection{Genetic Algorithms}
Genetic algorithms are a family of search algorithms inspired by the principles of evolution in nature. By imitating the process of natural selection and reproduction, genetic algorithms can produce high-quality solutions for various problems involving search, optimization, and learning. At the same time, their analogy to natural evolution allows genetic algorithms to overcome some of the hurdles that are encountered by traditional search and optimization algorithms, especially for problems with a large number of parameters and complex mathematical representations. Thus, they come in handy for optimizing our networks.
In order to make use of genetic algorithms we must decide some components, which are:
\begin{itemize}
\item Genotype
\item Population
\item Fitness Function
\item Selection Algorithm
\item Crossover Algorithm
\item Mutation Algorithm
\item Elitism
\end{itemize}
All of these components are implemented using the python library \textbf{deap}\footnote{https://deap.readthedocs.io/en/master/}.

\subsubsection{Genotype}
The \textit{genotype} is a collection of genes that are grouped into chromosomes. In our specific case, our genes are bounded real-valued encoded and they represent three different parameters:
\begin{itemize}
\item \textit{activation\_function}: bounded between 0 and 1.999. The integer part stands for ReLU (0) and ELU (1);
\item \textit{optimizer}: bounded between 0 and 1.999. The integer part stands for rmsprop (0) and adam (1);
\item \textit{learning rate}: bounded between 0.001 and 0.1.
\end{itemize}

\subsubsection{Population}
At any point in time, genetic algorithms maintain a population of individuals (i.e., chromosomes)– a collection of candidate solutions for the problem at hand. Since for each individual we train a model and evaluate its performance, we decided to use only 20 individuals per generation to limit the computation.

\subsubsection{Fitness Function}
At each iteration of the algorithm, the individuals are evaluated using a fitness function (also called the target function). This is the function we seek to optimize or the problem we attempt to solve.
In our problem, the fitness function is the the function which calculate the maximum validation accuracy reached by the model. Our objective is maximizing the validation accuracy.

\subsubsection{Selection Algorithm}
After calculating the fitness of every individual in the population, a selection process is used to determine which of the individuals in the population will get to reproduce and create the offspring that will form the next generation.
The selection algorithm used by as is \textit{tournament selection}. In each round of the tournament selection method, two individuals are randomly picked from the population, and the one with the highest fitness score wins and gets selected. We decided to select only two individuals since our population is small and selecting more could cause an abuse in exploitation.

\subsubsection{Crossover Algorithm}
To create a pair of new individuals, two parents are chosen from the current generation, and parts of their chromosomes are interchanged (crossed over) to create two new chromosomes representing the offspring. There exists different algorithms applicable to real-value encoded individuals, we decided to use the \textit{Simulated Binary Bounded Crossover}, which is a bounded version of the Simulated Binary Crossover (SBX)\footnote{https://content.wolfram.com/uploads/sites/13/2018/02/09-2-2.pdf}.

The idea behind the simulated binary crossover is to imitate the properties of the single-point crossover that is commonly used with binary-coded chromosomes. One of these properties is that the average of the parents' values is equal to that of the offsprings' values.
When applying SBX, the two offspring are created from the two parents using the following formula:

$
	offspring_1 = \frac{1}{2}[(1+\beta)parent_1 + (1-\beta)parent_2] 
$
\medskip

$
	offspring_2 = \frac{1}{2}[(1-\beta)parent_1 + (1+\beta)parent_2]
$


\noindent Here, $\beta$ is a random number referred to as the \textit{spread factor}.

\noindent This formula has the following notable properties:
\begin{itemize}
\item The average of the two offspring is equal to that of the parents, regardless of the value of $\beta$.
\item When the $\beta$ value is 1, the offspring are duplicates of the parents.
\item When the $\beta$ value is smaller than 1, the offspring are closer to each other than the parents were.
\item When the $\beta$ value is larger than 1, the offspring are farther apart from each other than the parents were.
\end{itemize}

\noindent The probability to mate is set equal to 0.9.

\subsubsection{Mutation Algorithm}
The purpose of the mutation operator is to periodically and randomly refresh the population, introduce new patterns into the chromosomes, and encourage search in uncharted areas of the solution space.
As mutation algorithm we decided to use the \textit{Polynomial Bounded} method, which is a bounded mutation operator that uses a polynomial function for the probability distribution.

\noindent The probability to mutate is set equal to 0.5.

\subsubsection{Elitism}
While the average fitness of the genetic algorithm population generally increases as generations go by, it is possible at any point that the best individual(s) of the current generation will be lost. This is due to the selection, crossover, and mutation operators altering the individuals in the process of creating the next generation. In many cases, the loss is temporary as these individuals (or better individuals) will be re-introduced into the population in a future generation.

However, if we want to guarantee that the best individual(s) always make it to the next generation, we can apply the optional elitism strategy. This means that the top \textit{n} individuals (\textit{n} being a small, predefined parameter, in our case 5) are duplicated into the next generation before we fill the rest of the available spots with offspring that are created using selection, crossover, and mutation. The elite individuals that were duplicated are still eligible for the selection process so they can still be used as the parents of new individuals.

Elitism is made possible in our code thanks to the function \textit{eaSimpleWithElitism}, which is a modification of the function \textit{eaSimple} present in the \textbf{Deap} framework.
